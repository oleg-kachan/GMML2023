{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometrical Methods in Machine Learning\n",
    "## Seminar 2: ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import math as m\n",
    "from scipy import signal\n",
    "from scipy.linalg import fractional_matrix_power as matrix_power\n",
    "\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn import image\n",
    "from nilearn.plotting import plot_stat_map, show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ICA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider ICA model:\n",
    "\n",
    "$$\\mathbf{X} = \\mathbf{A}\\mathbf{S}$$\n",
    "\n",
    "where $\\mathbf{S} \\in \\mathbb{R}^{n \\times d}$ - $n$ source signals of dimension $d$, $\\mathbf{X} \\in \\mathbb{R}^{m \\times d}$ - $m$ observations of dimension $d$, $\\mathbf{A}$ - $m \\times n$ mixing matrix, where $m \\geq n, \\mathrm{Rank}(\\mathbf{A}) = n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(66)\n",
    "S = np.random.logistic(0, 1, (2, 2000)) ** 3\n",
    "S = S / S.std()\n",
    "S[0, :] /= 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"$X_1$\")\n",
    "plt.hist(S[0,:], bins=50, density=True)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"$X_2$\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.hist(S[1,:], bins=50, density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Set mixing matrix $A = \\begin{pmatrix} 1 & 1 \\\\ 0 & 2 \\end{pmatrix}$ and define X as linear mixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set mixing matrix A\n",
    "A = # your code here\n",
    "\n",
    "# set X as linear mixture of S\n",
    "X = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(X.T)\n",
    "ica = FastICA(n_components=2).fit(X.T)\n",
    "print(\"PCA components:\\n\", pca.components_)\n",
    "print(\"\\nICA components:\\n\", ica.mixing_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_mixing_0 = ica.mixing_[:,0] / np.linalg.norm(ica.mixing_[:,0])\n",
    "ica_mixing_1 = ica.mixing_[:,1] / np.linalg.norm(ica.mixing_[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"True data\")\n",
    "plt.scatter(S.T[:,0], S.T[:,1], alpha=0.1)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Mixed data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.scatter(X.T[:,0], X.T[:,1], alpha=0.1)\n",
    "\n",
    "plt.quiver(0, 0, ica_mixing_0[0], ica_mixing_0[1], zorder=11, width=0.01, scale=5, color='r', alpha=0.8, label='ICA')\n",
    "plt.quiver(0, 0, ica_mixing_1[0], ica_mixing_1[1], zorder=11, width=0.01, scale=5, color='r', alpha=0.8)\n",
    "\n",
    "plt.quiver(0, 0, pca.components_[0,0], pca.components_[1,0], zorder=11, width=0.01, scale=5, color='orange', alpha=0.8, label='PCA')\n",
    "plt.quiver(0, 0, pca.components_[0,1], pca.components_[1,1], zorder=11, width=0.01, scale=5, color='orange', alpha=0.8)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data whitening\n",
    "\n",
    "Consider a generated dataset which is sampled from multivariate normal distribution with covariance matrix $C = \\begin{pmatrix} 5 & 3 \\\\ 3 & 2 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size\n",
    "sample_size = 1000\n",
    "\n",
    "# sample from multivariate Gaussian\n",
    "mu = np.random.normal(0, 0.5, 2)\n",
    "C = np.array([[5, 3], [3, 2]])\n",
    "data = np.random.multivariate_normal(mu, C, size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = (data[:,0] > 3) & (data[:,1] > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.title(\"Original data, mu={:.2f}, {:.2f}, var=1, 1\".format(mu[0], mu[1]))\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.scatter(data[:,0], data[:,1], alpha=0.1)\n",
    "plt.scatter(data[idx,0], data[idx,1], alpha=0.5)\n",
    "plt.scatter(mu[0], mu[1], alpha=1, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whitening procedure makes the samples made uncorrelated and their variances one.\n",
    "\n",
    "Whitening has two simple steps:\n",
    "\n",
    "1. (decorrelation) Project the dataset onto the eigenvectors (PCA). This rotates the dataset so that there is no correlation between the components.\n",
    "\n",
    "2. (standartization) Normalize the the dataset to have a variance of 1 for all components. This is done by simply dividing each component by the square root of its eigenvalue.\n",
    "\n",
    "Let $\\mathbf{S} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T$ is eigenvalue decomposition of sample covariance matrix $\\mathbf{S} \\triangleq \\mathbf{X}^T\\mathbf{X}$.\n",
    "\n",
    "Then whitening matrix $\\mathbf{W}$ is defined as:\n",
    "\n",
    "$$\\mathbf{W} = \\mathbf{V} \\mathbf{\\Lambda}^{-1/2}$$\n",
    "\n",
    "or alternatively (ZCA variant): \n",
    "\n",
    "$$\\mathbf{W}_{ZCA} = \\mathbf{V} \\mathbf{\\Lambda}^{-1/2} \\mathbf{V}^T$$\n",
    "\n",
    "Whitening transform is defined as:\n",
    "\n",
    "$$\\mathbf{X}' = \\mathbf{W} \\mathbf{X}$$\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Implement whitening matrix and whitening transform. Use either `scipy.linalg.fractional_matrix_power` or properties of diagonal matrix to raise it into power of $-1/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample mean\n",
    "x_mean = # your code here\n",
    "\n",
    "# center data\n",
    "data_centered = # your code here\n",
    "\n",
    "# compute covariance matrix\n",
    "covariance = # your code here\n",
    "print(\"Covariance matrix:\\n\", covariance)\n",
    "\n",
    "# obtain eigenvectors of covariance matrix\n",
    "eigenvalues, eigenvectors = # your code here\n",
    "print(\"\\nEigenvalues:\\n\", eigenvalues)\n",
    "print(\"\\nEigenvectors:\\n\", eigenvectors)\n",
    "\n",
    "# compute whitening matrix\n",
    "W_zca = # your code here\n",
    "print(\"\\nZCA whitening matrix:\\n\", W_zca)\n",
    "\n",
    "# compute PCA whitening matrix\n",
    "W = # your code here\n",
    "print(\"\\nPCA whitening matrix:\\n\", W)\n",
    "\n",
    "# scale data\n",
    "data_scaled = StandardScaler().fit_transform(data)\n",
    "\n",
    "# compute whitened data\n",
    "data_whitened = np.dot(data_centered, W)\n",
    "data_whitened_zca = np.dot(data_centered, W_zca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Check whether whitened data indeed have unit covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,4))\n",
    "\n",
    "plt.suptitle(\"Scaling and whitening\", fontsize=13)\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.xlim(-8, 8)\n",
    "plt.ylim(-8, 8)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Original data\")\n",
    "plt.scatter(data[:,0], data[:,1], alpha=0.2)\n",
    "plt.scatter(data[idx,0], data[idx,1], alpha=0.5)\n",
    "plt.scatter(mu[0], mu[1], alpha=1, c='r')\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Scaled data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.scatter(data_scaled[:,0], data_scaled[:,1], alpha=0.2)\n",
    "plt.scatter(data_scaled[idx,0], data_scaled[idx,1], alpha=0.5)\n",
    "plt.scatter(np.mean(data_scaled, axis=0)[0], np.mean(data_scaled, axis=0)[1], alpha=1, c='r')\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"PCA whitening\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.scatter(data_whitened[:,0], data_whitened[:,1], alpha=0.2)\n",
    "plt.scatter(data_whitened[idx,0], data_whitened[idx,1], alpha=0.5)\n",
    "plt.scatter(np.mean(data_whitened, axis=0)[0], np.mean(data_whitened, axis=0)[1], alpha=1, c='r')\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"ZCA whitening\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.scatter(data_whitened_zca[:,0], data_whitened_zca[:,1], alpha=0.2)\n",
    "plt.scatter(data_whitened_zca[idx,0], data_whitened_zca[idx,1], alpha=0.5)\n",
    "plt.scatter(np.mean(data_whitened_zca, axis=0)[0], np.mean(data_whitened_zca, axis=0)[1], alpha=1, c='r')\n",
    "\n",
    "plt.subplots_adjust(top = 0.82)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ICA solution\n",
    "\n",
    "### Kurtosis approach - make the output signal components most non-Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICA algorithm is conceptually relatively simple and works as follows:\n",
    "\n",
    "1. remove the mean and whiten (decorrelate and normalize covariance matrix to unit variance) the data\n",
    "2. rotate the data such that output signal components are most non-Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a linear mixture of two sources: $X_1 \\sim Laplace(0,1)$ and $X_2 \\sim U(0,1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample size and dimensionality\n",
    "n, d = 1000, 2\n",
    "\n",
    "X_1 = np.random.uniform(-np.sqrt(3),np.sqrt(3), (n))\n",
    "X_2 = np.random.laplace(0, 0.5, (n))\n",
    "X = np.vstack([X_1, X_2])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "#plt.xlim(-5, 5)\n",
    "#plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"$X_2 \\sim U(0,1)$\")\n",
    "plt.hist(X_1, bins=50, density=True)\n",
    "\n",
    "plt.subplot(122)\n",
    "#plt.xlim(-5, 5)\n",
    "#plt.ylim(-5, 5)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"$X_1 \\sim Laplace(0,1)$\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.hist(X_2, bins=50, density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "By defining a random mean and random mixing matrix $\\mathbf{M}$ obtain mixed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set random mean\n",
    "mean = # your code here\n",
    "\n",
    "# create random mixing matrix\n",
    "M = # your code here\n",
    "\n",
    "# mix initial sources and add mean.\n",
    "Y = # your code here\n",
    "\n",
    "print(\"Initial sources:\\n\", X)\n",
    "print(\"\\nInitial mean:\\n\", mean)\n",
    "print(\"\\nMixing matrix:\\n\", M)\n",
    "print(\"\\nMixed data:\\n\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Mixed data\")\n",
    "plt.scatter(Y[0], Y[1], alpha=0.2)\n",
    "plt.scatter(np.mean(Y.T, axis=0)[0], np.mean(Y.T, axis=0)[1], alpha=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white(data):\n",
    "    x_mean = np.mean(data, axis=1)\n",
    "    data_centered = data - x_mean.reshape((2, 1))\n",
    "    covariance = data_centered.dot(data_centered.T) / n\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance)\n",
    "    W = eigenvectors.dot(matrix_power(np.diag(eigenvalues), -0.5)).dot(eigenvectors.T)\n",
    "    return np.dot(W, data_centered), data_centered, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** whiten the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_white, Y_centered, W = white(Y)\n",
    "Y_white.shape, Y_centered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_white.dot(Y_white.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Whiten data\")\n",
    "plt.scatter(Y_white[0], Y_white[1], alpha=0.2)\n",
    "plt.scatter(np.mean(Y_white, axis=1)[0], np.mean(Y_white, axis=1)[1], alpha=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing measure of non-Gaussianity\n",
    "\n",
    "Loss function (_contrast_ for historical reasons) is defined as:\n",
    "\n",
    "$$C(X) := \\sum_i \\mathrm{kurt}(X)^2$$\n",
    "\n",
    "Where _kurthosis_ (corrected four-order moment or cumulant) of (multivariate) random variable $X$ is defined:\n",
    "\n",
    "$$\\mathrm{kurt}(X) = \\mathbb{E}(X^4) - 3\\mathbb{E}(X^2)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Code the kurtosis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kurtosis(data_centered):\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    return kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kurtosis(Y_white))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "The contrast function is a scalar function that quantifies on the basis of fourth order cumulants how non-Gaussian the components of a signal are.\n",
    "\n",
    "Code the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(data_centered):\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    return contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss(Y_white))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "**Step 2:** Find optimal rotation angle with random Givens rotations.\n",
    "\n",
    "In higher dimensions a rotation matrix can be quite complex. To keep things simple, we use Givens rotations, which are defined as a rotation within the 2D subspace spanned by two axes n and m. For instance a  rotation matrix in 2D is given by:\n",
    "\n",
    "$$\\mathbf{R} = \\begin{pmatrix} cos \\phi & sin \\phi \\\\ -sin \\phi & cos \\phi \\end{pmatrix}$$\n",
    "\n",
    "A rotation within the plane spanned by the second and the fourth axis ($n$ = 2, $m$ = 4) of a four-dimensional space is given by\n",
    "\n",
    "$$\\mathbf{R} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0  & cos \\phi & 0 & sin \\phi \\\\ 0 & 0 & 1 & 0 \\\\ 0 & -sin \\phi & 0 & cos \\phi \\end{pmatrix}$$\n",
    "\n",
    "It can be shown that any general rotation, i.e. any orthogonal matrix with positive determinant, can be written as a product of Givens-rotations. Thus, we can find the general rotation matrix of the ICA problem by applying a series of Givens rotations and each time improve the objective function. By applying enough of such Givens-rotations, the algorithm should eventually converge to the globally optimal solution.\n",
    "\n",
    "This is a simple procedure to rotate the data such that the contrast function is maximized, i.e. such that the components are maximally non-Gaussian, which implies that the components are minimally statistically independent (as far as fourth order is concerned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with rotation angle 0, i.e. with rotation matrix [[1,0],[0,1]] and contrast of the whitened data.\n",
    "\n",
    "bestAngle = 0\n",
    "bestRotationMatrix = np.array([[1, 0], [0, 1]])\n",
    "bestRotatedContrast = loss(Y_white)\n",
    "\n",
    "losses = []\n",
    "\n",
    "angle = 0\n",
    "for iteration in np.arange(0, 10, 0.01):\n",
    "\n",
    "    # Try a new rotation angle near the old one.\n",
    "    \n",
    "    # Vary the angle a bit, where 'a bit' gets exponentially smaller\n",
    "    angle = angle + np.random.normal(0, m.exp(-iteration))\n",
    "    \n",
    "    # Calculate new rotation matrix\n",
    "    rotationMatrix = np.array([[m.cos(angle), -m.sin(angle)], [m.sin(angle), m.cos(angle)]])\n",
    "    \n",
    "    # Calculate rotated whitened data\n",
    "    rotatedData = np.dot(rotationMatrix, Y_white)\n",
    "    \n",
    "    # Calculate contrast ov newly rotated whitened data\n",
    "    rotatedContrast = loss(rotatedData)\n",
    "    \n",
    "    # If contrast has improved, keep the new rotation angle, matrix, and contrast, otherwise continue with the old ones.\n",
    "    if rotatedContrast > bestRotatedContrast:\n",
    "        \n",
    "        bestAngle = angle\n",
    "        bestRotationMatrix = rotationMatrix\n",
    "        bestRotatedContrast = rotatedContrast\n",
    "        \n",
    "        print(\"Loss (max): {:.4f}\".format(rotatedContrast))\n",
    "        \n",
    "    losses.append(rotatedContrast)\n",
    "        \n",
    "# Display final result.\n",
    "print(\"\\nBest angle: \", bestAngle)\n",
    "print(\"\\nBest rotation matrix:\\n\", bestRotationMatrix)\n",
    "print(\"\\nBest rotated contrast: \", bestRotatedContrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Loss function\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Find unmixing matrix $\\mathbf{U} = \\mathbf{R} \\mathbf{W}$ and separated sources $\\mathbf{S} = \\mathbf{U} \\mathbf{\\bar{Y}}$, where $\\mathbf{\\bar{Y}}$ is centered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unmixing matrix U\n",
    "U = # your code here\n",
    "\n",
    "# find separated sources S\n",
    "S = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.xlabel(\"$S_1$\")\n",
    "plt.ylabel(\"$S_2$\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Separated data\")\n",
    "plt.scatter(S[0], S[1], alpha=0.2)\n",
    "plt.scatter(np.mean(S, axis=1)[0], np.mean(S, axis=1)[1], alpha=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Plot the distribution of separated sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "#plt.xlim(-5, 5)\n",
    "#plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Separated 1\")\n",
    "plt.hist(S[0], bins=50, density=True)\n",
    "\n",
    "plt.subplot(122)\n",
    "#plt.xlim(-5, 5)\n",
    "#plt.ylim(-5, 5)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.title(\"Separated 2\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.hist(S[1], bins=50, density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Blind source separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set image size\n",
    "shape = (512, 512)\n",
    "rows, cols = shape\n",
    "\n",
    "# load images\n",
    "img1 = np.load('./data/camera.npy').flatten()\n",
    "img2 = np.load('./data/astronaut.npy').flatten()\n",
    "img3 = np.load('./data/moon.npy').flatten()\n",
    "img4 = np.load('./data/noise.npy').flatten()\n",
    "\n",
    "# combine images\n",
    "S = np.c_[img1, img2, img3, img4].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random mixing matrix A\n",
    "A = np.random.uniform(0.01, 0.9, (4, 4))\n",
    "# mix data\n",
    "X = np.dot(A, S)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images\n",
    "f, ax = plt.subplots(1, X.shape[0])\n",
    "f.set_size_inches((16,4))\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    ax[i].imshow(X[i,:].reshape(rows, cols), cmap=plt.gray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Run ICA with different number of `n_components`. Compare results, conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ICA\n",
    "ica = # your code here\n",
    "img_ica = # your code here\n",
    "img_ica.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images\n",
    "f, ax = plt.subplots(1, img_ica.shape[1])\n",
    "f.set_size_inches((16,4))\n",
    "\n",
    "for i in range(img_ica.shape[1]):\n",
    "    ax[i].imshow(img_ica[:,i].reshape(shape), cmap=plt.gray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "time = np.linspace(0, 8, n_samples)\n",
    "\n",
    "s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n",
    "s2 = np.sign(np.sin(3.5 * time))  # Signal 2 : square signal\n",
    "s3 = signal.sawtooth(1.5 * np.pi * time) # sawtooteh\n",
    "\n",
    "S = np.c_[s1, s2, s3]\n",
    "#S += np.array([0, 0, np.random.normal()])  # Add noise\n",
    "\n",
    "S /= S.std(axis=0)  # Standardize data\n",
    "\n",
    "# Mix data\n",
    "A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix\n",
    "#A = np.random.normal(0, 1, (3, 3))\n",
    "X = np.dot(S, A.T)  # Generate observations\n",
    "\n",
    "# Compute ICA\n",
    "ica = FastICA(n_components=3)\n",
    "S_ = ica.fit_transform(X)  # Reconstruct signals\n",
    "A_ = ica.mixing_  # Get estimated mixing matrix\n",
    "\n",
    "# We can `prove` that the ICA model applies by reverting the unmixing.\n",
    "#assert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)\n",
    "\n",
    "# For comparison, compute PCA\n",
    "pca = PCA(n_components=3)\n",
    "H = pca.fit_transform(X)  # Reconstruct signals based on orthogonal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "models = [X, S, S_, H]\n",
    "names = ['Observations (mixed signal)',\n",
    "         'True Sources',\n",
    "         'ICA recovered signals',\n",
    "         'PCA recovered signals']\n",
    "colors = ['red', 'steelblue', 'orange']\n",
    "\n",
    "for ii, (model, name) in enumerate(zip(models, names), 1):\n",
    "    plt.subplot(4, 1, ii)\n",
    "    plt.title(name)\n",
    "    for sig, color in zip(model.T, colors):\n",
    "        plt.plot(sig, color=color)\n",
    "\n",
    "plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.46)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. fMRI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fMRI signal is represented by a $t \\times v$ data matrix $\\mathbf{X}$, where $t$ is the number of time points and $v$ is the number of voxels in the volumes. This means that each row of $\\mathbf{S}$ contains an independent spatial pattern and the corresponding column of $\\mathbf{A} \\in \\mathbb{R}^{t \\times t}$ holds its activation time-course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"img/fmri.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch patient data\n",
    "dataset = datasets.fetch_adhd(n_subjects=1)\n",
    "func_filename = dataset.func[0]\n",
    "\n",
    "# extract time series\n",
    "masker = NiftiMasker(smoothing_fwhm=8, memory='nilearn_cache', detrend=True, standardize=True)\n",
    "fmri_data = masker.fit_transform(func_filename)\n",
    "\n",
    "fmri_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Plot every 1000-th voxel activation time course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,4))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.xlabel(\"Time step\")\n",
    "\n",
    "for i in range(0, fmri_data.shape[1], 1000):\n",
    "    plt.plot(fmri_data[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Fit ICA to fMRI data to extract desired number of independent components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 20\n",
    "\n",
    "# apply ICA on spatial domain\n",
    "ica = FastICA(n_components=n_components, random_state=42)\n",
    "components = ica.fit_transform(fmri_data.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize estimated components, for thresholding to make sense\n",
    "components -= components.mean(axis=0)\n",
    "components /= components.std(axis=0)\n",
    "\n",
    "# threshold\n",
    "components[np.abs(components) < 1.0] = 0\n",
    "\n",
    "# now invert the masking operation, projecting to original space\n",
    "component_img = masker.inverse_transform(components)\n",
    "\n",
    "# plot image\n",
    "mean_img = image.mean_img(func_filename)\n",
    "\n",
    "for i in range(n_components):\n",
    "    plot_stat_map(image.index_img(component_img, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Fit PCA to fMRI data to extract desired number of principal components. Compare the result to ICA, conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
