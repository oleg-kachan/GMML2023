{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometrical Methods in Machine Learning\n",
    "## Seminar 5: Topological Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade Cython\n",
    "!pip install --upgrade Ripser\n",
    "!pip install --upgrade persim\n",
    "!pip install --upgrade diagram2vec\n",
    "!pip install --upgrade giotto-tda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ripser import lower_star_img\n",
    "from ripser import Rips\n",
    "vr = Rips()\n",
    "\n",
    "import persim\n",
    "import diagram2vec\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "import simplicial\n",
    "import simplicial.drawing\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.diagrams import PersistenceImage, PersistenceEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Simplicial complexes and homology\n",
    "\n",
    "**Exercise**: create a simplicial complex consisting of 7 vertices and 6 edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create simplicial complex\n",
    "cmplx = simplicial.SimplicialComplex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 0-simplices (vertices)\n",
    "v1 = cmplx.addSimplex(id=\"v1\")\n",
    "v2 = cmplx.addSimplex(id=\"v2\")\n",
    "v3 = cmplx.addSimplex(id=\"v3\")\n",
    "v4 = cmplx.addSimplex(id=\"v4\")\n",
    "v5 = cmplx.addSimplex(id=\"v5\")\n",
    "v6 = cmplx.addSimplex(id=\"v6\")\n",
    "v7 = cmplx.addSimplex(id=\"v7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 1-simplices (edges)\n",
    "cmplx.addSimplex(['v2', 'v3'], id=\"e1\")\n",
    "cmplx.addSimplex(['v4', 'v5'], id=\"e2\")\n",
    "cmplx.addSimplex(['v4', 'v6'], id=\"e3\")\n",
    "cmplx.addSimplex(['v5', 'v6'], id=\"e4\")\n",
    "cmplx.addSimplex(['v5', 'v7'], id=\"e5\")\n",
    "cmplx.addSimplex(['v6', 'v7'], id=\"e6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set coordinates for vertices\n",
    "em = simplicial.Embedding(cmplx)\n",
    "em.positionSimplex(v1, (0.0, 0.5))\n",
    "\n",
    "em.positionSimplex(v2, (0.25, 1.0))\n",
    "em.positionSimplex(v3, (0.25, 0.0))\n",
    "\n",
    "em.positionSimplex(v4, (1.0, 1.0))\n",
    "em.positionSimplex(v5, (0.5, 0.66))\n",
    "em.positionSimplex(v6, (1.0, 0.33))\n",
    "em.positionSimplex(v7, (0.5, 0.0))\n",
    "\n",
    "# draw simplicial complex\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.title(\"Geometric realization of simplicial complex\")\n",
    "simplicial.drawing.draw_complex(cmplx, em)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Betti numbers\n",
    "\n",
    "_K-th Betti number_ of topological space is the rank of its' associated _k-th homological group_ and describes the number of holes of dimension $k+1$.\n",
    "\n",
    "- 0-dimensional holes are connected components\n",
    "- 1-dimensional holes are loops\n",
    "- 2-dimensional holes are voids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Evaluate the dimensionality and non-vanishing Betti numbers of a simplicial complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate complex dimension\n",
    "dim = cmplx.maxOrder()\n",
    "\n",
    "# evaluate Betti numbers\n",
    "betti = cmplx.bettiNumbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension of complex: {}\".format(dim))\n",
    "for key in betti:\n",
    "    if key <= dim:\n",
    "        print(\"Betti-{}: {}\".format(str(key), betti[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the topology of the simplicial complex\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Modify the topology of the simplicial complex by \"filling\" one of its' 2-dimensional holes by a 2-simplex which is a triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 2-simplex (triangle)\n",
    "cmplx.addSimplex(['e2', 'e3', 'e4'], id=\"t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw simplicial complex\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.title(\"Geometric realization of simplicial complex\")\n",
    "simplicial.drawing.draw_complex(cmplx, em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate complex dimension\n",
    "dim = cmplx.maxOrder()\n",
    "\n",
    "# evaluate Betti numbers\n",
    "betti = cmplx.bettiNumbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension of complex: {}\".format(dim))\n",
    "for key in betti:\n",
    "    if key <= dim:\n",
    "        print(\"Betti-{}: {}\".format(str(key), betti[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Persistent diagrams, Wasserstein distance and stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data\n",
    "X, y = make_circles(n_samples=200, noise=0.1)\n",
    "X = X[y==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topology studies data invariant to continous transformations, so topological invariants like (persistent) homology will not change under such class of transformations.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Apply rotation and dilation transformations to copy of original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.radians(30)\n",
    "c, s = np.cos(theta), np.sin(theta)\n",
    "R = np.array(((c,-s), (s, c)))\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed data\n",
    "X_transformed = np.copy(X)\n",
    "X_transformed[:,0] = X[:,0] * 0.75\n",
    "X_transformed = np.dot(X_transformed, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Data\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.33)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Transformed data\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1], alpha=0.33)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Compute persistence diagrams of a filtration of Vietoris-Rips complex built on point cloud data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram = vr.fit_transform(X)\n",
    "diagram_transformed = vr.fit_transform(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.suptitle(\"Persistent diagram of a filtration\")\n",
    "\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Data\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "vr.plot(diagram)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Transformed data\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "vr.plot(diagram_transformed)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can define the geometry on the space of persistent diagrams, defining a metric on it. Optimal transport approach is used to compare persistent diagrams which are multisets of intervals of arbitrary cardinality.  \n",
    "\n",
    "The variants of optimal transport distances are _Wasserstein-2 distance_, and its approximations like _sliced Wasserstein distance_ and _Bollteneck distance_, which is Wasserstein-$\\infty$ distance.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Compute Bottleneck `persim.bottleneck` and sliced Wasserstein distances `persim.sliced_wasserstein` between perisistent diagrams of original and transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram_transformed[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persim.bottleneck(diagram[1], diagram_transformed[1]) # pc w noise=0.1 vs w/ noise 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persim.sliced_wasserstein(diagram[1], diagram_transformed[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottlneck distance used a single matching between most discriminative pair of points.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Visualize Bottleneck matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Bottleneck distance matching\n",
    "d, matching = persim.bottleneck(diagram[1], diagram_transformed[1], matching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Bottleneck distance matchign\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.suptitle(\"Bottleneck distance matching\")\n",
    "persim.bottleneck_matching(diagram[1], diagram_transformed[1], matching, labels=['Original $H_1$', 'Transformed $H_1$'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Bottleneck distance stability to small perturbations is theoretically proved.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Plot Bottleneck distance with respect to different level of Gaussian noise applied to original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data\n",
    "X_orig, y = make_circles(n_samples=500, noise=0.0)\n",
    "X_orig = X_orig\n",
    "diagram_orig = vr.fit_transform(X_orig)\n",
    "\n",
    "# your code here\n",
    "dists = []\n",
    "\n",
    "for noise in tqdm(np.arange(0, 0.26, 0.02)):\n",
    "    # your code here\n",
    "    X_noisy, _ = make_circles(n_samples=500, noise=noise)\n",
    "    X_noisy = X_noisy\n",
    "    \n",
    "    diagram_noise = vr.fit_transform(X_noisy)\n",
    "    \n",
    "    dists.append(persim.bottleneck(diagram_orig[1], diagram_noise[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Persistent homology of graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline is as follows:\n",
    "\n",
    "1. compute persistent diagrams via Ripser \n",
    "2. compute vectorization of diagrams, so-called persistent images and Betti curves\n",
    "3. apply classifier on vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_graphs = pickle.load(open(\"./data/metric_graphs/X.pkl\", \"rb\"))\n",
    "y = pickle.load(open(\"./data/metric_graphs/y_all.pkl\", \"rb\"))\n",
    "y_dnod = pickle.load(open(\"./data/metric_graphs/y_d_nod.pkl\", \"rb\"))\n",
    "\n",
    "y_col = [\"a\"] * len(y)\n",
    "y_col = np.array(y_col)\n",
    "\n",
    "y_col[y==0] = \"blue\"\n",
    "y_col[y==2] = \"green\"\n",
    "y_col[y==1] = \"red\"\n",
    "y_col[y==3] = \"yellow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute persistent diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add h_1 diagrams only\n",
    "maxdim = 1\n",
    "h = 1\n",
    "\n",
    "rips = Rips(maxdim=maxdim)\n",
    "\n",
    "diagrams = []\n",
    "for x in X_graphs:\n",
    "    diagrams.append(rips.fit_transform(x, distance_matrix=True)[h])\n",
    "\n",
    "len(diagrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = len(X_graphs)\n",
    "distances = np.zeros((n, n))\n",
    "\n",
    "for i in range(0, n):\n",
    "    for j in range(i+1, n):\n",
    "        distances[i,j] = persim.sliced_wasserstein(diagrams[i], diagrams[j])\n",
    "        \n",
    "distances_symmetrize = distances + distances.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_symmetrize = distances + distances.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, max_iter=3000, eps=1e-9, dissimilarity=\"precomputed\", random_state=1, n_jobs=-1)\n",
    "X_metric = mds.fit(distances_symmetrize).embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.scatter(X_metric[:, 0], X_metric[:, 1], c=y_col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "Persistent diagram is a multiset of intervals of arbitrary length which is can not be handled by methods of machine learning.\n",
    "\n",
    "#### Persistent images\n",
    "\n",
    "One possible to solutions besides providing a metric on the space of persistent diagrams is vectorization of diagrams to a vector of fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vietorisrips_tr = VietorisRipsPersistence(metric=\"precomputed\")\n",
    "diagrams_giotto = vietorisrips_tr.fit_transform(X_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = PersistenceImage(sigma=0.025, n_bins=64)\n",
    "pimages = pi.fit_transform(diagrams_giotto)\n",
    "pimages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(12,4))\n",
    "\n",
    "ax[0].set_title(\"Persistent diagram\")\n",
    "rips.plot(diagrams[0], legend=False, ax=ax[0])\n",
    "\n",
    "ax[1].set_title(\"Persistent image, dim=0\")\n",
    "ax[1].imshow(pimages[0,0])\n",
    "\n",
    "ax[2].set_title(\"Persistent image, dim=1\")\n",
    "ax[2].imshow(pimages[0,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = pimages[:,1].reshape((pimages[:,1].shape[0], -1))\n",
    "y_all = pickle.load(open(\"./data/metric_graphs/y_all.pkl\", \"rb\")).astype(int)\n",
    "\n",
    "X_control = X_all[y_all==0]\n",
    "X_depression = X_all[y_all==1]\n",
    "X = np.concatenate((X_control, X_depression), axis=0)\n",
    "y = np.concatenate((np.zeros(25), np.ones(25)), axis=0)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, y_train = X[train_index], y[train_index]\n",
    "    X_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "    model = LogisticRegression(penalty='l2', C=10.0, solver='liblinear', random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracies.append(model.score(X_test, y_test))\n",
    "\n",
    "print(\"Accuracy: {:.4f} ± {:.4f}\".format(np.mean(accuracies), np.std(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betti curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_betti_curve = diagram2vec.persistence_curve(diagrams, m=32)\n",
    "X_betti_curve.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_betti_curve.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "ax = plt.gca()\n",
    "plt.xlim(0,1)\n",
    "plt.title(\"Betti curve\", pad=15)\n",
    "plt.xlabel(\"t\", fontsize=12, ha=\"right\", x=1)\n",
    "plt.ylabel(\"\", ha=\"right\", y=1)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.step(np.linspace(0,1,32), X_betti_curve[0,0], color=\"b\", where=\"post\", linewidth=2, label=\"Pers-1\")\n",
    "plt.step(np.linspace(0,1,32), X_betti_curve[0,1], color=\"r\", where=\"post\", linewidth=2, label=\"Pers-2\")\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_control = X_betti_curve[0][y_all==0]\n",
    "X_depression = X_betti_curve[0][y_all==1]\n",
    "X = np.concatenate((X_control, X_depression), axis=0)\n",
    "y = np.concatenate((np.zeros(25), np.ones(25)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, y_train = X[train_index], y[train_index]\n",
    "    X_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracies.append(model.score(X_test, y_test))\n",
    "\n",
    "print(\"Accuracy: {:.4f} ± {:.4f}\".format(np.mean(accuracies), np.std(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Persistent homology of digital images\n",
    "\n",
    "Persistence Diagrams with Linear Machine Learning Models (Obayashi, Hiraoka), 2017  \n",
    "https://arxiv.org/abs/1706.10082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 300\n",
    "sigma1 = 4\n",
    "sigma2 = 2\n",
    "t = 0.01\n",
    "\n",
    "def generate(N, S, W=300, sigma1=4, sigma2=2, t=0.01, bins=64):\n",
    "\n",
    "    z = np.zeros((N, S, 2))\n",
    "    for n in range(N):\n",
    "        z[n, 0] = np.random.uniform(0, W, size=(2))\n",
    "        for s in range(S-1):\n",
    "            d_1 = np.random.normal(0, sigma1)\n",
    "            d_2 = np.random.normal(0, sigma1)\n",
    "            z[n, s+1, 0] = (z[n, s, 0] + d_1) % W\n",
    "            z[n, s+1, 1] = (z[n, s, 1] + d_2) % W\n",
    "\n",
    "    z_r = z.reshape(N*S, 2)\n",
    "    H, _, _ = np.histogram2d(z_r[:,0], z_r[:,1], bins=bins)\n",
    "    \n",
    "    G = gaussian_filter(H, sigma2)\n",
    "    G[G < t] = 0\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation\n",
    "\n",
    "Generate 100 images accoring to model A and model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.zeros((100,64,64))\n",
    "\n",
    "# class A\n",
    "N = 100\n",
    "S = 30\n",
    "\n",
    "for n in range(50):\n",
    "    images[n] = generate(N, S)\n",
    "    \n",
    "# class B\n",
    "N = 250\n",
    "S = 10\n",
    "\n",
    "for n in range(50):\n",
    "    images[n+50] = generate(N, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.gray()\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.title(\"Class A\")\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "plt.title(\"Class B\")\n",
    "\n",
    "ax1.imshow(images[int(np.random.uniform(0, 50))])\n",
    "ax2.imshow(images[int(np.random.uniform(51, 100))])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute persistent diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diags = []\n",
    "\n",
    "for i in range(images.shape[0]):\n",
    "    diags.append(lower_star_img(images[i])[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "persim.plot_diagrams(diags[52])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pi = persim.PersImage(spread=0.04, pixels=[32, 32], verbose=False)\n",
    "#pers_images = np.array(pi.transform(diags))\n",
    "\n",
    "betti_curves = diagram2vec.persistence_curve(diags, m=25)\n",
    "\n",
    "#pers_images.shape, betti_curves.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betti curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_betti_curves = betti_curves[0]\n",
    "y = np.concatenate((np.zeros(50), np.ones(50)), axis=0)\n",
    "\n",
    "y_col = [\"b\"] * len(y)\n",
    "y_col = np.array(y_col)\n",
    "\n",
    "y_col[y==1] = \"r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in skf.split(X_betti_curves, y):\n",
    "    X_train, y_train = X_betti_curves[train_index], y[train_index]\n",
    "    X_test, y_test = X_betti_curves[test_index], y[test_index]\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracies.append(model.score(X_test, y_test))\n",
    "\n",
    "print(\"Accuracy: {:.4f} ± {:.4f}\".format(np.mean(accuracies), np.std(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Compute the two-dimensional embeddings using linear and nonlinear techniques learned during the course, given persistent images, Betti curves and pairwise distances between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, SpectralEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb = Isomap().fit_transform(X_betti_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.scatter(X_emb[:, 0], X_emb[:, 1], c=y_col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deep sets\n",
    "\n",
    "#### Problem\n",
    "Persistence diagram is a multiset of vectors $D = \\{(b_i, d_i, h_i)\\}_{i=1}^N$ where $b_i$, $d_i$ are the birth and death times of $i$-th topological feature of dimension $h_i$. The classic approach to introduce persistent diagrams to machine learning is related to distances and kernels defined on the space of diagrams, which takes $O(n^2)$ time to compute. Vectrorization schemes such as persistence [images](https://arxiv.org/abs/1507.06217), [landscapes](https://arxiv.org/abs/1501.00179) or [curves](https://arxiv.org/abs/1904.07768) reduce the time to $O(n)$, yet all of this approaches are more or less fixed.\n",
    "\n",
    "Trainable vectorization allows to learn vector representations of persistence diagrams, optimal w.r.t. the downstream task such as classification or regression. The simplest of such models, [Deep Sets](https://arxiv.org/abs/1703.06114) - $f: (\\mathbb{R}^3)^N \\rightarrow \\mathbb{R}^d$\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\{x_1, \\dots, x_N\\}) = \\rho \\left( \\sum_{i=1}^N \\phi(x_i) \\right),\n",
    "\\end{equation}\n",
    "\n",
    "consists of an encoder $\\phi_\\theta: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^D$ mapping each diagram point $x_i = (b_i, d_i, h_i)$, with parameters $\\theta$ shared between points, a permutation invariant pooling operation $(\\cdot): (\\mathbb{R}^D)^N \\rightarrow \\mathbb{R}^D$ to obtain a representation of a diagram at whole (particulary for Deep Sets - sum pooling), and a decoder $\\rho: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d$ which further transforms the diagram representation. It was [shown](https://arxiv.org/abs/1904.09378) that certain combinations of encoder/pooling/decoder correspond to the fixed representation schemes of persistence diagrams.\n",
    "\n",
    "Deep sets encoder vectorizes each single point independently and does not consider the interdependence between points in the diagram. Thus, the self-attention block from the Transformer model which allows to capture those dependencies is a natural plug-in replacement to the encoder $\\phi$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\Phi_{ATTN}(\\{x_1, \\dots, x_N\\}) = \\left(\\frac{(\\mathbf{W}_q \\mathbf{X})(\\mathbf{W}_k \\mathbf{X})^T}{\\sqrt{D}} \\right)\\mathbf{W}_v\\mathbf{X},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Phi_{ATTN}: (\\mathbb{R}^3)^N \\rightarrow (\\mathbb{R}^D)^N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_orbit(point_0, r, n=300):\n",
    "    \n",
    "    X = np.zeros([n, 2])\n",
    "    \n",
    "    xcur, ycur = point_0[0], point_0[1]\n",
    "    \n",
    "    for idx in range(n):\n",
    "        xcur = (xcur + r * ycur * (1. - ycur)) % 1\n",
    "        ycur = (ycur + r * xcur * (1. - xcur)) % 1\n",
    "        X[idx, :] = [xcur, ycur]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def generate_orbits(m, rs=[2.5, 3.5, 4.0, 4.1, 4.3], n=300, random_state=None):\n",
    "    \n",
    "    # m orbits, each of n points of dimension 2\n",
    "    orbits = np.zeros((m * len(rs), n, 2))\n",
    "    \n",
    "    # for each r\n",
    "    for j, r in enumerate(rs):\n",
    "\n",
    "        # initial points\n",
    "        points_0 = random_state.uniform(size=(m,2))\n",
    "\n",
    "        for i, point_0 in enumerate(points_0):\n",
    "            orbits[j*m + i] = generate_orbit(points_0[i], rs[j])\n",
    "            \n",
    "    return orbits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(42)\n",
    "X_orbit5k = generate_orbits(1000, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(16, 6), dpi=200)\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        if i==0:\n",
    "            ax[i,j].set_title(\"Class {}\".format(j+1))\n",
    "        ax[i,j].scatter(X_orbit5k[j*1000+i,:,0], X_orbit5k[j*1000+i,:,1], s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pc = np.concatenate((X_orbit5k[2000:3000], X_orbit5k[4000:5000]))\n",
    "y = np.concatenate((np.zeros(1000), np.ones(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_pd(diagrams):\n",
    "    pd = np.zeros((0, 3))\n",
    "\n",
    "    for k, diagram_k in enumerate(diagrams):\n",
    "        diagram_k = diagram_k[~np.isinf(diagram_k).any(axis=1)] # filter infs  \n",
    "        diagram_k = np.concatenate((diagram_k, k * np.ones((diagram_k.shape[0], 1))), axis=1)\n",
    "        pd = np.concatenate((pd, diagram_k))\n",
    "\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for x_pc in tqdm(X_pc):\n",
    "    diagram = conv_pd(vr.fit_transform(x_pc))\n",
    "    X.append(diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orbit2kDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "def collate_fn(data):\n",
    "    \n",
    "    tmp_pd, _ = data[0]\n",
    "    \n",
    "    n_batch = len(data)\n",
    "    n_features_pd = tmp_pd.shape[1]\n",
    "    n_points_pd = max(len(pd) for pd, _ in data)\n",
    "    inputs_pd = np.zeros((n_batch, n_points_pd, n_features_pd), dtype=float)\n",
    "    labels = np.zeros(len(data))\n",
    "    \n",
    "    for i, (pd, label) in enumerate(data):\n",
    "        inputs_pd[i][:len(pd)] = pd\n",
    "        labels[i] = label\n",
    "    \n",
    "    return torch.Tensor(inputs_pd), torch.Tensor(labels).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSets(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden_enc, n_out_enc, n_hidden_dec=16, n_out_dec=2):\n",
    "        super(DeepSets, self).__init__()\n",
    "        self.encoder = Encoder(n_in, n_hidden_enc, n_out_enc)\n",
    "        self.decoder = MLP(n_out_enc, n_hidden_dec, n_out_dec)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        z_enc = self.encoder(X)\n",
    "        z = self.decoder(z_enc)\n",
    "        return z\n",
    "    \n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = Linear(n_in, n_hidden)\n",
    "        self.linear2 = Linear(n_hidden, n_out)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = relu(self.linear1(X))\n",
    "        X = self.linear2(X)\n",
    "        return X\n",
    "    \n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.mlp = MLP(n_in, n_hidden, n_out)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.mlp(X)\n",
    "        x = X.mean(dim=1) # aggregation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_repeats = 3\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "\n",
    "n_train, n_test = 1600, 400\n",
    "\n",
    "history = np.zeros((n_repeats, n_epochs, 3))\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "dataset = Orbit2kDataset(X, y)\n",
    "\n",
    "for repeat_idx in range(n_repeats):\n",
    "    \n",
    "    # data init\n",
    "    dataset_train, dataset_test = random_split(dataset, [n_train, n_test])\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloader_test =  DataLoader(dataset_test, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # model init\n",
    "    model = DeepSets(n_in=3, n_hidden_enc=32, n_out_enc=16)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(\"{:3} {:6} {:6} {:6}\".format(repeat_idx, \"Loss\", \"Train\", \"Test\"))\n",
    "    \n",
    "    for epoch_idx in range(n_epochs):\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        \n",
    "        loss_epoch = []\n",
    "        for batch in dataloader_train:\n",
    "            loss_batch = criterion(model(batch[0]), batch[1])\n",
    "            loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_epoch.append(loss_batch.detach())\n",
    "        \n",
    "        loss_epoch_mean = np.array(loss_epoch).mean()\n",
    "        history[repeat_idx,epoch_idx,0] = loss_epoch_mean\n",
    "        \n",
    "        # test\n",
    "        model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        for batch in dataloader_train:\n",
    "            y_hat = model(batch[0]).argmax(dim=1)\n",
    "            correct += int((y_hat == batch[1]).sum())\n",
    "        accuracy_train = correct / len(dataloader_train.dataset)\n",
    "        history[repeat_idx,epoch_idx,1] = accuracy_train\n",
    "\n",
    "        correct = 0\n",
    "        for batch in dataloader_test:\n",
    "            y_hat = model(batch[0]).argmax(dim=1)\n",
    "            correct += int((y_hat == batch[1]).sum())\n",
    "        accuracy_test = correct / len(dataloader_test.dataset)\n",
    "        history[repeat_idx,epoch_idx,2] = accuracy_test\n",
    "        \n",
    "        print(\"{:3} {:.4f} {:.4f} {:.4f}\".format(epoch_idx, loss_epoch_mean, accuracy_train, accuracy_test))\n",
    "    print(\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(n_epochs)\n",
    "loss_ci1 = history.mean(axis=0)[:,0] - history.std(axis=0)[:,0]\n",
    "loss_ci2 = history.mean(axis=0)[:,0] + history.std(axis=0)[:,0]\n",
    "acc_train_ci1 = history.mean(axis=0)[:,1] - history.std(axis=0)[:,1]\n",
    "acc_train_ci2 = history.mean(axis=0)[:,1] + history.std(axis=0)[:,1]\n",
    "acc_test_ci1 = history.mean(axis=0)[:,2] - history.std(axis=0)[:,2]\n",
    "acc_test_ci2 = history.mean(axis=0)[:,2] + history.std(axis=0)[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 4.5), dpi=200)\n",
    "fig.suptitle(\"Persistence diagrams\", fontsize=14)\n",
    "ax[0].set_title(\"Loss\")\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "ax[0].set_ylim(0.58, 0.7)\n",
    "ax[1].set_ylim(0.45, 0.75)\n",
    "ax[0].plot(history.mean(axis=0)[:,0], c=\"g\")\n",
    "ax[0].fill_between(x, loss_ci1, loss_ci2, color=\"g\", alpha=0.1)\n",
    "ax[1].plot(history.mean(axis=0)[:,1], c=\"r\", label=\"Train\")\n",
    "ax[1].plot(history.mean(axis=0)[:,2], c=\"b\", label=\"Test\")\n",
    "ax[1].fill_between(x, acc_train_ci1, acc_train_ci2, color=\"r\", alpha=0.1)\n",
    "ax[1].fill_between(x, acc_test_ci1, acc_test_ci2, color=\"b\", alpha=0.1)\n",
    "ax[1].legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
